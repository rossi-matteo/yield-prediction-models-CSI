{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBT\n",
    "\n",
    "This Python Notebook is divided in three sections, in order to remain inside the RAM constraints given by the Kaggle cell. \\\n",
    "In order to execute this script successfully through \"Save Version -> Save and Run All (Commit)\" in background without having to avoid the Idle Timeout of 40 minutes given by the platform, you will have to comment out all the cells in the other sections to not execute them.\n",
    "\n",
    "1. Preprocessing of Training Data and Train the Model:\n",
    "   importing the DataFrames, compute the stats chosen to train the model, train the model (3h) and save it in '/kaggle/working'\n",
    "2. Preprocessing of Test Data:\n",
    "   same features selected on Training Dataset and same stats, this operation has to be done separately since the Test Set is huge.\n",
    "3. Generate Predictions and Submission.csv\n",
    "   generating the future yields (3h).\n",
    "\n",
    "The first three cells **do not have** to be commented in any of these steps. {imports, definitions os global vars, reduce_memory_usage function}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scores**\n",
    "* RMSE [2021-2050]: 1.330\n",
    "* RMSE [2051-2098]: 1.448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-25T16:38:36.806318Z",
     "iopub.status.busy": "2024-12-25T16:38:36.805992Z",
     "iopub.status.idle": "2024-12-25T16:38:40.666517Z",
     "shell.execute_reply": "2024-12-25T16:38:40.665598Z",
     "shell.execute_reply.started": "2024-12-25T16:38:36.806293Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import re, gc, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "gc.enable()\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:42:48.821989Z",
     "iopub.status.busy": "2024-12-25T16:42:48.821651Z",
     "iopub.status.idle": "2024-12-25T16:42:48.828082Z",
     "shell.execute_reply": "2024-12-25T16:42:48.826874Z",
     "shell.execute_reply.started": "2024-12-25T16:42:48.821965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CROPS = (\"maize\", \"wheat\")\n",
    "\n",
    "MODES = ('train', 'test')\n",
    "\n",
    "FEATURES_TEMPORAL = {\n",
    "    # Time series data -- 240 columns reflecting daily values for 30 days before sowing and 210 days after.\n",
    "    'tas',       # Mean daily temperature\n",
    "    'tasmax',    # Max daily temperature\n",
    "    'tasmin',    # Min daily temperature\n",
    "    'pr',        # precipitation\n",
    "    'rsds'      # shortwave radiation\n",
    "}\n",
    "\n",
    "FEATURES_STATIC = {\n",
    "    # Static data\n",
    "    'soil_co2',  # crop, year, lon, lat, texture_class, real_year, co2, nitrogen\n",
    "    # dominant USDA soil texture class (constant over time), the ambient CO2 concentration (spatially constant), the planting date and the nitrogen application rate (constant over time)\n",
    "}\n",
    "\n",
    "FEATURES = set.union(FEATURES_TEMPORAL, FEATURES_STATIC)\n",
    "\n",
    "COLUMNS_TO_DROP = ['crop','variable']\n",
    "\n",
    "# Sowing date\n",
    "INDEX_SOW = 30  # days\n",
    "# Time series data length\n",
    "SEASON_LENGTH = 240  # days\n",
    "# Nr. of soil texture classes\n",
    "NUM_TEXTURE_CLASSES = 13  \n",
    "\n",
    "YEAR_TRAIN_MIN = 1982\n",
    "YEAR_TRAIN_MAX = 2020  # Inclusive\n",
    "YEAR_TEST_MIN = 2021\n",
    "YEAR_TEST_MAX = 2098\n",
    "\n",
    "PATH_INPUT = os.path.abspath(os.path.join(os.sep, 'kaggle', 'input', 'the-future-crop-challenge'))\n",
    "# PATH_INPUT = os.path.abspath(os.path.join(os.getcwd(), 'data'))  # For running the notebook locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:43:04.163534Z",
     "iopub.status.busy": "2024-12-25T16:43:04.163186Z",
     "iopub.status.idle": "2024-12-25T16:43:04.173477Z",
     "shell.execute_reply": "2024-12-25T16:43:04.172415Z",
     "shell.execute_reply.started": "2024-12-25T16:43:04.163492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reduce memory usage of a pandas DataFrame\n",
    "def reduce_memory_usage(df):\n",
    "    \"\"\"Reduce memory usage of a pandas DataFrame.\"\"\"\n",
    "    # Function to iterate through columns and modify the data types\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    #print(f\"Memory usage of dataframe: {start_mem} MB\")\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in df.index.names:  # Skip index columns, since other formats of index aren't supported by the engine\n",
    "            continue\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)  # Keep sufficient precision\n",
    "            else:\n",
    "                if col == \"year\":  # Ensure precision for grouping columns\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                if col == 'lat' or 'lon':\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                elif c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    #print(f\"Memory usage after optimization: {end_mem} MB\")\n",
    "    #print(f\"Decreased by {100 * (start_mem - end_mem) / start_mem}%\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T12:29:20.763130Z",
     "iopub.status.busy": "2024-12-25T12:29:20.762737Z",
     "iopub.status.idle": "2024-12-25T12:29:20.771218Z",
     "shell.execute_reply": "2024-12-25T12:29:20.769866Z",
     "shell.execute_reply.started": "2024-12-25T12:29:20.763102Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(crop: str, # Which crop\n",
    "              mode: str, # Which dataset (i.e. train/test)\n",
    "              select_only_features: bool = True # Drop every other column (crop, year, lon, lat) if not relevant for computation\n",
    "             ) -> dict:\n",
    "    assert crop in CROPS\n",
    "    assert mode in MODES\n",
    "    \n",
    "    output = dict()\n",
    "    \n",
    "    for f in FEATURES:\n",
    "        path = os.path.join(PATH_INPUT, f'{f}_{crop}_{mode}.parquet')\n",
    "        df = reduce_memory_usage(pd.read_parquet(path))\n",
    "\n",
    "        columns_to_drop_in_df = [col for col in COLUMNS_TO_DROP if col in df.columns] \n",
    "        if columns_to_drop_in_df:\n",
    "            df = df.drop(columns=columns_to_drop_in_df)\n",
    "\n",
    "        if select_only_features:\n",
    "            if f in FEATURES_TEMPORAL:  # Select only the time series data -- drop other columns\n",
    "                df = df[[str(i) for i in range(SEASON_LENGTH)]]\n",
    "        \n",
    "        output[f] = df\n",
    "\n",
    "        del df  # Explicitly delete the DataFrame\n",
    "        gc.collect()  # Force garbage collection\n",
    "        \n",
    "    if mode == 'train':\n",
    "        output['target'] = pd.read_parquet(os.path.join(PATH_INPUT, f'{mode}_solutions_{crop}.parquet'))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T12:29:32.297800Z",
     "iopub.status.busy": "2024-12-25T12:29:32.297340Z",
     "iopub.status.idle": "2024-12-25T12:30:28.759246Z",
     "shell.execute_reply": "2024-12-25T12:30:28.757315Z",
     "shell.execute_reply.started": "2024-12-25T12:29:32.297766Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "crop_data_train = {\n",
    "    crop: load_data(crop, 'train', select_only_features=False) for crop in CROPS\n",
    "}\n",
    "\n",
    "crop_features_train = {\n",
    "    crop: {\n",
    "        k: v for k, v in data.items() if k in FEATURES\n",
    "    } for crop, data in crop_data_train.items()\n",
    "}\n",
    "\n",
    "crop_targets_train = {\n",
    "    crop: data['target'] for crop, data in crop_data_train.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T12:31:03.443463Z",
     "iopub.status.busy": "2024-12-25T12:31:03.443037Z",
     "iopub.status.idle": "2024-12-25T12:31:15.637324Z",
     "shell.execute_reply": "2024-12-25T12:31:15.636082Z",
     "shell.execute_reply.started": "2024-12-25T12:31:03.443399Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['year', 'lon', 'lat']\n",
    "\n",
    "for crop in CROPS:\n",
    "    for input_feature in crop_features_train[crop].keys():\n",
    "        if input_feature == 'soil_co2':\n",
    "            continue\n",
    "        crop_features_train[crop][input_feature] = crop_features_train[crop][input_feature].drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_statistics_on_crop(df):    \n",
    "    #We already have the vars splitte\n",
    "    #Consider relying on tasmax, tasmin instead of these aggregated stats\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mean_tas = df['tas'].mean(axis=1).rename('mean_tas')\n",
    "    median_tas = df['tas'].median(axis=1).rename('median_tas')\n",
    "    sum_tas = df['tas'].sum(axis=1).rename('sum_tas')\n",
    "    min_tas = df['tas'].min(axis=1).rename('min_tas')\n",
    "    max_tas = df['tas'].max(axis=1).rename('max_tas')\n",
    "    \n",
    "    mean_pr = df['pr'].mean(axis=1).rename('mean_pr')\n",
    "    median_pr = df['pr'].median(axis=1).rename('median_pr')\n",
    "    sum_pr = df['pr'].sum(axis=1).rename('sum_pr')\n",
    "    min_pr = df['pr'].min(axis=1).rename('min_pr')\n",
    "    max_pr = df['pr'].max(axis=1).rename('max_pr')\n",
    "    \n",
    "    mean_rsds = df['rsds'].mean(axis=1).rename('mean_rsds')\n",
    "    median_rsds = df['rsds'].median(axis=1).rename('median_rsds')\n",
    "    sum_rsds = df['rsds'].sum(axis=1).rename('sum_rsds')\n",
    "    min_rsds = df['rsds'].min(axis=1).rename('min_rsds')\n",
    "    max_rsds = df['rsds'].max(axis=1).rename('max_rsds')\n",
    "\n",
    "    # Create a DataFrame to store the results\n",
    "    summary_df = pd.concat([mean_tas, min_tas, max_tas, median_tas, sum_tas,\n",
    "                            mean_pr, min_pr, max_pr, median_pr, sum_pr,\n",
    "                            mean_rsds, median_rsds, sum_rsds, min_rsds, max_rsds], axis=1)\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for crop in CROPS:\n",
    "    crop_features_train[crop]['summary'] = reduce_memory_usage(calculate_statistics_on_crop(crop_features_train[crop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_chunk_statistics(crop_data: dict, x: int) -> pd.DataFrame:\n",
    "    # Categories and their prefixes\n",
    "    categories = ['pr', 'tas', 'rsds', 'tasmax', 'tasmin']\n",
    "    statistics = ['mean', 'median', 'sum', 'max', 'min']\n",
    "    \n",
    "    # Initialize an empty DataFrame to store the results\n",
    "    chunk_summary_df = pd.DataFrame()\n",
    "\n",
    "    for category in categories:\n",
    "        \n",
    "        # Split the columns into chunks of size x\n",
    "        for i in range(0, crop_data[category].shape[1], x):  # Iterate over column indices\n",
    "            chunk_columns = crop_data[category].iloc[:, i:i + x]  # Slice columns\n",
    "            \n",
    "            if not chunk_columns.empty:\n",
    "                chunk_stats = {\n",
    "                    f'{category}_{stat}_chunk_{i//x}': getattr(chunk_columns, stat)(axis=1)\n",
    "                    for stat in statistics\n",
    "                }\n",
    "                \n",
    "                chunk_summary_df = pd.concat([chunk_summary_df, pd.DataFrame(chunk_stats)], axis=1)\n",
    "                \n",
    "                del chunk_columns, chunk_stats\n",
    "                gc.collect()\n",
    "\n",
    "    return chunk_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for crop in CROPS:\n",
    "    crop_features_train[crop]['chunk_stats'] = reduce_memory_usage(calculate_chunk_statistics(crop_features_train[crop], 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T12:31:47.507237Z",
     "iopub.status.busy": "2024-12-25T12:31:47.506885Z",
     "iopub.status.idle": "2024-12-25T12:31:47.655806Z",
     "shell.execute_reply": "2024-12-25T12:31:47.654677Z",
     "shell.execute_reply.started": "2024-12-25T12:31:47.507211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Merging yields\n",
    "for crop in CROPS:\n",
    "    crop_features_train[crop]['soil_co2'] = crop_features_train[crop]['soil_co2'].merge(\n",
    "        crop_targets_train[crop],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T12:31:50.799124Z",
     "iopub.status.busy": "2024-12-25T12:31:50.798735Z",
     "iopub.status.idle": "2024-12-25T12:31:50.862411Z",
     "shell.execute_reply": "2024-12-25T12:31:50.861158Z",
     "shell.execute_reply.started": "2024-12-25T12:31:50.799094Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Adding mean_yield to the Training Features, since it's a strong predictor.\n",
    "\n",
    "for crop in CROPS:\n",
    "    crop_features_train[crop]['soil_co2']['mean_yield'] = crop_features_train[crop]['soil_co2'].groupby(['lat', 'lon'])['yield'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine features for both crops\n",
    "X_maize = pd.concat([\n",
    "    crop_features_train['maize']['soil_co2'].drop(columns=['yield']),\n",
    "    crop_features_train['maize']['summary'],\n",
    "    crop_features_train['maize']['chunk_stats']\n",
    "], axis=1)\n",
    "\n",
    "X_wheat = pd.concat([\n",
    "    crop_features_train['wheat']['soil_co2'].drop(columns=['yield']),\n",
    "    crop_features_train['wheat']['summary'],\n",
    "    crop_features_train['wheat']['chunk_stats']\n",
    "], axis=1)\n",
    "\n",
    "# Combine targets for both crops\n",
    "y_maize = crop_features_train['maize']['soil_co2']['yield']\n",
    "y_wheat = crop_features_train['wheat']['soil_co2']['yield']\n",
    "\n",
    "# Optionally add a crop type column\n",
    "X_maize['crop_type'] = 'maize'\n",
    "X_wheat['crop_type'] = 'wheat'\n",
    "\n",
    "# Combine into a single dataset\n",
    "X_combined = pd.concat([X_maize, X_wheat], axis=0)\n",
    "y_combined = pd.concat([y_maize, y_wheat], axis=0)\n",
    "\n",
    "# Encode crop type as a category\n",
    "X_combined['crop_type'] = X_combined['crop_type'].astype('category')\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_combined, y_combined, test_size=0.2, random_state=42)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 100000,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "}\n",
    "\n",
    "# Create LightGBM datasets\n",
    "trainset = lgb.Dataset(X_train, label=y_train)\n",
    "val_dataset = lgb.Dataset(X_val, label=y_val, reference=trainset)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    trainset,\n",
    "    valid_sets=[trainset, val_dataset],\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate Metrics\n",
    "rmse = mean_squared_error(y_val, y_pred, squared=False)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "print(f\"R2: {r2}\")\n",
    "\n",
    "model.save_model('/kaggle/working/lightgbm_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Feature Importances\n",
    "feature_importance_df = pd.DataFrame()\n",
    "feature_importance_df['Feature'] = X_train.columns\n",
    "feature_importance_df['Importance'] = model.feature_importance(importance_type='gain')\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Plotting the top 50 features by importance\n",
    "top_n = 50\n",
    "top_features = feature_importance_df.iloc[:top_n]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], align='center', color='skyblue')\n",
    "plt.xlabel('Feature Importance (Gain)')\n",
    "plt.ylabel('Features')\n",
    "plt.title(f'Top {top_n} Features Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "del X_maize, X_wheat, y_maize, y_wheat, X_val, X_combined, y_combined, y_train, y_val, trainset, val_dataset, crop_data_train, crop_features_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF SECTION 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2 - Test Set Preprocessing and split into Parquets\n",
    "\n",
    "Remember that you will need also the first three cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "crop_data_test = {\n",
    "    crop: load_data(crop, 'test', select_only_features=False) for crop in CROPS\n",
    "}\n",
    "\n",
    "crop_features_test = {\n",
    "    crop: {\n",
    "        k: v for k, v in data.items() if k in FEATURES\n",
    "    } for crop, data in crop_data_test.items()\n",
    "}\n",
    "\n",
    "# Directory to store intermediate Parquet files\n",
    "output_dir = \"/kaggle/working/\"\n",
    "\n",
    "columns_to_drop = ['year', 'lon', 'lat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for crop in CROPS:\n",
    "    for input_feature in crop_features_test[crop].keys():\n",
    "        if input_feature == 'soil_co2':\n",
    "            continue\n",
    "        crop_features_test[crop][input_feature] = crop_features_test[crop][input_feature].drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Calculate summary statistics and chunk stats\n",
    "    crop_summary = reduce_memory_usage(calculate_statistics_on_crop(crop_features_test[crop]))\n",
    "\n",
    "    crop_chunk_stats = reduce_memory_usage(calculate_chunk_statistics(crop_features_test[crop], 30))\n",
    "\n",
    "    # Combine the features for the crop\n",
    "    X_test_crop = pd.concat([crop_summary, crop_chunk_stats], axis=1)\n",
    "\n",
    "    # Save the processed crop data to a Parquet file\n",
    "    parquet_file = os.path.join(output_dir, f\"{crop}_processed.parquet\")\n",
    "    X_test_crop.to_parquet(parquet_file)\n",
    "\n",
    "    print(f\"Saved {crop} data to {parquet_file}\")\n",
    "\n",
    "    del crop_summary, crop_chunk_stats, X_test_crop\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3 - Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:43:09.683597Z",
     "iopub.status.busy": "2024-12-25T16:43:09.683278Z",
     "iopub.status.idle": "2024-12-25T16:43:32.096919Z",
     "shell.execute_reply": "2024-12-25T16:43:32.095400Z",
     "shell.execute_reply.started": "2024-12-25T16:43:09.683571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATH_INPUT = os.path.abspath(os.path.join(os.sep, 'kaggle', 'input', 'the-future-crop-challenge'))\n",
    "PATH_TRAIN_DATA = os.path.abspath(os.path.join(os.sep, 'kaggle', 'input', 'fcc-train-data-stats'))\n",
    "PATH_WORKING = os.path.abspath(os.path.join(os.sep, 'kaggle', 'working'))\n",
    "\n",
    "# Prepare test data for predictions\n",
    "X_test = pd.DataFrame()\n",
    "\n",
    "for crop in CROPS:\n",
    "\n",
    "    crop_soil_co2_test = reduce_memory_usage(pd.read_parquet(os.path.join(PATH_INPUT, f'soil_co2_{crop}_test.parquet')))\n",
    "    crop_stats = reduce_memory_usage(pd.read_parquet(os.path.join(PATH_TRAIN_DATA, f'{crop}_processed.parquet')))\n",
    "\n",
    "    # Concatenate features for the crop\n",
    "    X_test_crop = pd.concat([\n",
    "        crop_soil_co2_test,\n",
    "        crop_stats\n",
    "    ], axis=1)\n",
    "\n",
    "    del crop_soil_co2_test, crop_stats\n",
    "    gc.collect()\n",
    "    \n",
    "    X_test_crop['crop_type'] = crop\n",
    "    X_test_crop['crop_type'] = X_test_crop['crop_type'].astype('category')\n",
    "\n",
    "    X_test = pd.concat([X_test, X_test_crop])\n",
    "\n",
    "    del X_test_crop\n",
    "    gc.collect()                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:56:21.216012Z",
     "iopub.status.busy": "2024-12-25T16:56:21.215583Z",
     "iopub.status.idle": "2024-12-25T16:56:21.477226Z",
     "shell.execute_reply": "2024-12-25T16:56:21.476095Z",
     "shell.execute_reply.started": "2024-12-25T16:56:21.215981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Here crop_features_train[crop]['soil_co2']['mean_yield'] we have the yield of the crops.\n",
    "I have to link them with latitude and longitude to the grid cells in X_test, we can merge them on 'lat' and 'lon'\n",
    "'''\n",
    "crop_features_train = {\n",
    "    crop: reduce_memory_usage(pd.read_parquet(os.path.join(PATH_INPUT, f'soil_co2_{crop}_train.parquet')))\n",
    "    for crop in CROPS\n",
    "}\n",
    "\n",
    "# Load training targets\n",
    "crop_targets_train = {\n",
    "    crop: pd.read_parquet(os.path.join(PATH_INPUT, f'train_solutions_{crop}.parquet'))\n",
    "    for crop in CROPS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:56:23.378902Z",
     "iopub.status.busy": "2024-12-25T16:56:23.378548Z",
     "iopub.status.idle": "2024-12-25T16:56:23.504404Z",
     "shell.execute_reply": "2024-12-25T16:56:23.503387Z",
     "shell.execute_reply.started": "2024-12-25T16:56:23.378872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for crop in CROPS:\n",
    "    crop_features_train[crop] = crop_features_train[crop].merge(\n",
    "        crop_targets_train[crop],\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "for crop in CROPS:\n",
    "    crop_features_train[crop]['mean_yield'] = crop_features_train[crop].groupby(['lat', 'lon'])['yield'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:56:26.717456Z",
     "iopub.status.busy": "2024-12-25T16:56:26.717107Z",
     "iopub.status.idle": "2024-12-25T16:56:30.464124Z",
     "shell.execute_reply": "2024-12-25T16:56:30.463101Z",
     "shell.execute_reply.started": "2024-12-25T16:56:26.717428Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combine all crop features into one DataFrame\n",
    "all_crop_features = pd.concat([\n",
    "    crop_features_train[crop][['lat', 'lon', 'mean_yield']] for crop in CROPS\n",
    "], ignore_index=True)\n",
    "\n",
    "# Drop duplicates to avoid redundant data\n",
    "all_crop_features = all_crop_features.drop_duplicates(subset=['lat', 'lon'])\n",
    "\n",
    "# Merge with X_test\n",
    "X_test = X_test.merge(all_crop_features, on=['lat', 'lon'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T16:56:35.536416Z",
     "iopub.status.busy": "2024-12-25T16:56:35.536081Z",
     "iopub.status.idle": "2024-12-25T16:56:42.809056Z",
     "shell.execute_reply": "2024-12-25T16:56:42.807688Z",
     "shell.execute_reply.started": "2024-12-25T16:56:35.536391Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features used during training: ['crop_type']\n"
     ]
    }
   ],
   "source": [
    "PATH_MODEL = os.path.abspath(os.path.join(os.sep, 'kaggle', 'input', 'fcc-lgbt/scikitlearn/v1.0/1', 'model'))\n",
    "model = lgb.Booster(model_file=os.path.join(PATH_MODEL, 'lightgbm_model.txt'))\n",
    "\n",
    "feature_names = model.feature_name()\n",
    "\n",
    "X_test = X_test[feature_names]\n",
    "\n",
    "categorical_indices = model.params.get('categorical_feature', [])\n",
    "\n",
    "categorical_features = [X_test.columns[i] for i in categorical_indices]\n",
    "print(\"Categorical features used during training:\", categorical_features)\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in X_test.columns:\n",
    "        X_test[feature] = X_test[feature].astype('category')\n",
    "\n",
    "for col in X_test.columns:\n",
    "    if X_test[col].dtype == 'object' and col not in categorical_features:\n",
    "        X_test[col] = pd.to_numeric(X_test[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T13:11:00.873108Z",
     "iopub.status.busy": "2024-12-25T13:11:00.872700Z",
     "iopub.status.idle": "2024-12-25T15:29:16.410760Z",
     "shell.execute_reply": "2024-12-25T15:29:16.402295Z",
     "shell.execute_reply.started": "2024-12-25T13:11:00.873081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Make predictions using the trained model\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Number of Rows needed\n",
    "print(len(predictions) == 1245149)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(os.path.join(PATH_INPUT, f'sample_submission.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T15:39:38.186614Z",
     "iopub.status.busy": "2024-12-25T15:39:38.186215Z",
     "iopub.status.idle": "2024-12-25T15:39:38.201545Z",
     "shell.execute_reply": "2024-12-25T15:39:38.200074Z",
     "shell.execute_reply.started": "2024-12-25T15:39:38.186582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Format predictions into submission format\n",
    "submission = pd.DataFrame({\n",
    "    'ID': sample_submission['ID'],\n",
    "    'yield': predictions\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T15:39:40.851011Z",
     "iopub.status.busy": "2024-12-25T15:39:40.850641Z",
     "iopub.status.idle": "2024-12-25T15:39:43.164152Z",
     "shell.execute_reply": "2024-12-25T15:39:43.162897Z",
     "shell.execute_reply.started": "2024-12-25T15:39:40.850985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save combined submission to CSV\n",
    "submission.to_csv('/kaggle/working/lgbt_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8812083,
     "sourceId": 81000,
     "sourceType": "competition"
    },
    {
     "datasetId": 6371819,
     "sourceId": 10295094,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 201033,
     "modelInstanceId": 178737,
     "sourceId": 209639,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
